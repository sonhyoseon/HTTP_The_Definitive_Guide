# 웹 로봇
웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
많은 로봇이 웹 사이트에서 다른 웹 사이트로 떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다.

## 9.1 크롤러와 크롤링
웹 크롤러는 먼저 웹 페이지를 한 개 가져오고 그 다음 그 페이지가 가져오는 모든 웹 페이지를 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
웹 링크를 재귀적으로 반복하는 로봇을 크롤러 혹은 스파이더라 부른다.
인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다. 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어지는데
이는 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

### 9.1.1 어디에서 시작하는가: '루트 집합'
굶주린 크롤러를 풀어놓기 전에, 우선 출발지점을 주어야 한다.
크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 불린다.
루트 집합을 고를 때는 여러 웹페이지에서 자료를 가져올 수 있도록 충분히 다른 장소를 선택해야 한다.

웹의 몇몇 페이지들은 그들로 향하는 어떤 링크도 없이 고립되어 있다.

일반적으로 좋은 루트 집합은 크고 인기있는 웹 사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.
많은 대규모 크롤러 제품들은 사용자들이 루트 집합에 새 페이지나 잘 알려지지 않는 페이지들을 추가하는 기능을 제공한다.
### 9.1.2 링크  추출과 상대 링크 정상화
크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다.
크롤러는 검색한 각 페이지 안에 있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다.
크롤로가 크롤링을 진행하며 탐색해야 할 링크를 발견함에 따라 이 목록은 급격히 증가한다.

### 9.1.3 순환 피하기
로봇이 웹을 크롤링 할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 한다.
로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 한다.

### 9.1.4 루프와 중복
순환은 최소한 아래 세가지 이유로 크롤러에게 해롭다.
- 순환은 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있다.
- 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 된다.
- 비록 루프 자체가 문제가 되지 않더라도 크롤러는 많은 수의 중복된 페이지를 가져오게 된다.

### 9.1.5 빵 부스러기의 흔적
불행히도 방문한 곳을 지속적으로 추적하기는 쉽지 않다.
만약 전 세계의 웹 콘텐츠의 상당 부분을 크롤링 하려면 수십억개의 URL을 방문할 준비가 필요할 것이다.
수십억 개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요하다.
대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법을 몇 가지 살펴보면 다음과 같다.

***트리와 해시 테이블***
복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다. 이들은 URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료 구조이다.

***느슨한 존재 비트맵***
공간 사용을 최소화하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용한다.
각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는  '존재 비트'를 갖는다.
URL이 크롤링 되었을 때, 해당하는 존재 비트가 만들어진다.
***체크포인트***
로봇 프로그램이 갑작스럽게 중단될 경우를 대비해 방문한 URL의 목록이 디스크에 저장되어있는지 확인한다.

***파티셔닝***
웹이 성장하면서, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다.
몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 '농장'을 이용한다.
각 로봇엔 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 진다.


### 9.1.6 별칭과 로봇 순환
올바른 자료 구조를 갖추었더라고 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주는 게 쉽지 않을 때도 있다.
한 URL이 또 다른 URL에 대한 별칭이라면 그 둘이 서도 달라 보이더라도 사실은 같은 리소스를 가리키고 있다.

### 9.1.7 URL 정규화하기
대부분의 웹 로봇은 URL들을 표준 형식으로 정규화 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다.
로봇은 아래와 같은 방법으로 모든 URL을 정규화된 형식으로 변환할 수 있다.

1. 포트 번호가 명시되지 않았다면 호스트 명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
3. # 태그들을 제거한다.

URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만 로봇들은 URL을 표준 형식으로 변환하는 것 만드로는 제거할 수 없는 다른 별칭을 만나게 될 것이다.

### 9.1.8 파일 시스템 랭크 순환
파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에 매우 교묘한 종류의 순환을 유발할 수 있다.


### 9.1.9 동적 가상 웹 공간
악의적인 웹 마스터들이 로봇들을 함정으로 빠뜨리기 위해 의도적으로 복잡한 크롤러 루프를 만드는 것은 있을 수 있는 일이다.

### 9.1.10 루프와 중복 피하기
모든 순환을 피하는 완벽한 방법은 없다.
잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
웹은 로봇이 문제를 일으킬 가능성으로 가득 차 있다.
이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.

***URL 정규화***
URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피한다.

***너비 우선 크롤링***
크롤러들은 언제든지 크롤링을 할 수 있는 URL들의 큰 집합을 갖고 있다.
방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면,
순환의 영향을 최소화 할 수 있다.

***스로틀링***
로봇이 웹 사이트에서 일정시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.

***URL 크기 제한***
로봇은 일정 길이를 넘는 URL의 크롤링은 거부할 수 있다.
만약 순환으로 인해 URL이 계속해서 길어진다면, 결국에는 길이 제한으로 인해 순환이 중단될 것이다.
주의해야 할 점은, 이 기법을 사용하면 가져오지 못하는 콘텐츠들도 틀림없이 있을 것이라는 점이다.

***URL/사이트 블랙리스트***
로봇 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL의 목록을 만들어 관리하고 그들을 전염병 피하듯 피한다.
문제를 일으키는 사이트나 URL이 발견될 때마다 이 블랙리스트에 추가한다.
***패턴 발견***
파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다.
***콘텐츠 지문***
지문은 더욱 복잡한 웹 크롤러들 몇몇에 의해 사용되는 중복을 감지하는 보다 직접적인 방법이다.
콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다. 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페이지의 링크는 크롤링 하지 않는다.
***사람의 모니터링***
로봇은 결국 어떤 기법으로도 해결할 수 없는 문제에 봉착할 것이다.
이러한 문제는 사람이 직접 제어해 주어야 한다.

## 9.2 로봇의 HTTP
로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않다.
그들 또한 HTTP 명세의 규칙을 지켜야 한다.
많은 로봇이 그들이 찾는 콘텐츠를 요청하기 위해 필요한 HTTP를 최소한으로만 구현하려 하기 때문에 많은 로봇이 HTTP/1.0 요청을 보낸다.

### 9.2 1 요청 헤더 식별하기
로봇들이 HTTP를 최소한도로만 지원하려고 함에도 불구하고 그들 대부분은 약간의 신원 식별 헤더를 구현하고 전송한다.
로봇 구현자들은 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇가지 헤더를 사이트에게 보내주는 것이 좋다.
이는 잘못된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려 할 때 모두 유용한 정보이다.
기본적인 식별 헤더들에는 다음과 같은 것이 있다.
- User-Agent
- From
- Accept
- Referer

### 9.2.2 가상 호스팅
로봇 구현자들은 Host 헤더를 지원할 필요가 있다.
가상 호스팅이 널리 퍼져있는 현실에서
요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.

대부분의 서버들은 기본적으로 특정 사이트 하나를 운영하도록 설정되어 있다.
따라서 Host 헤더를 포함하지 않은 크롤러는 두 개의 사이트를 운영하는 서버에 요청을 보내면
각각 다른 콘텐츠가 아닌 동일한 콘텐츠를 얻을 것이다.

### 9.2.3 조건부 요청
때때로 로봇들이 많은 양의 요청을 시도한다는 것을 고려할 때, 로봇이 검색하는 콘텐츠의 양을 최소화 하는 것은 상당히 의미 있는 일이다.
수십억 개의 웹페이지를 다운 받게 될 수도 있는 인터넷 검색엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것은 의미가 있다.

### 9.2.4 응답 다루기
대다수 로봇들은 주 관심사가 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에
응답 다루기라고 부를만한 일은 거의 하지 않지만
특정 몇몇 기능을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알 필요가 있다.

***상태 코드***
일반적으로 로봇들은 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.

또한 모든 서버가 언제나 항상 적절한 에러 코드를 반환하지 않는다는 걸 알아 두어야 한다.
***엔티티***
HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔티티 자체에 정보를 찾을 수 있다.
메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보이다.
이 태그는 수신자가 문서를 마치 그 문서의 HTTP 응답 값이 요청 헤더인
 Refresh 헤더를 포함하고 있는 것처럼 다루게 한다.


### 9.2.5 User-Agent 타겟팅
웹 관리자들은 많은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고,
그 로봇들로부터의 요청을 예상해야 한다.
많은 웹 사이트들은 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화한다.
이렇게 함으로써 사이트는 로봇에게는 콘텐츠 대신 에러 페이지를 제공한다.

사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다.
사이트 관리자들은 최소한 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록 대비해야 한다.
