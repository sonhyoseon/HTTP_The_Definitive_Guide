# 웹 로봇
웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
많은 로봇이 웹 사이트에서 다른 웹 사이트로 떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다.

## 9.1 크롤러와 크롤링
웹 크롤러는 먼저 웹 페이지를 한 개 가져오고 그 다음 그 페이지가 가져오는 모든 웹 페이지를 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
웹 링크를 재귀적으로 반복하는 로봇을 크롤러 혹은 스파이더라 부른다.
인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다. 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어지는데
이는 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

### 9.1.1 어디에서 시작하는가: '루트 집합'
굶주린 크롤러를 풀어놓기 전에, 우선 출발지점을 주어야 한다.
크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 불린다.
루트 집합을 고를 때는 여러 웹페이지에서 자료를 가져올 수 있도록 충분히 다른 장소를 선택해야 한다.

웹의 몇몇 페이지들은 그들로 향하는 어떤 링크도 없이 고립되어 있다.

일반적으로 좋은 루트 집합은 크고 인기있는 웹 사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.
많은 대규모 크롤러 제품들은 사용자들이 루트 집합에 새 페이지나 잘 알려지지 않는 페이지들을 추가하는 기능을 제공한다.
### 9.1.2 링크  추출과 상대 링크 정상화
크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다.
크롤러는 검색한 각 페이지 안에 있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다.
크롤로가 크롤링을 진행하며 탐색해야 할 링크를 발견함에 따라 이 목록은 급격히 증가한다.

### 9.1.3 순환 피하기
로봇이 웹을 크롤링 할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 한다.
로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 한다.

### 9.1.4 루프와 중복
순환은 최소한 아래 세가지 이유로 크롤러에게 해롭다.
- 순환은 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있다.
- 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 된다.
- 비록 루프 자체가 문제가 되지 않더라도 크롤러는 많은 수의 중복된 페이지를 가져오게 된다.

### 9.1.5 빵 부스러기의 흔적
불행히도 방문한 곳을 지속적으로 추적하기는 쉽지 않다.
만약 전 세계의 웹 콘텐츠의 상당 부분을 크롤링 하려면 수십억개의 URL을 방문할 준비가 필요할 것이다.
수십억 개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요하다.
대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법을 몇 가지 살펴보면 다음과 같다.

***트리와 해시 테이블***
복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다. 이들은 URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료 구조이다.

***느슨한 존재 비트맵***
공간 사용을 최소화하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용한다.
각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는  '존재 비트'를 갖는다.
URL이 크롤링 되었을 때, 해당하는 존재 비트가 만들어진다.
***체크포인트***
로봇 프로그램이 갑작스럽게 중단될 경우를 대비해 방문한 URL의 목록이 디스크에 저장되어있는지 확인한다.

***파티셔닝***
웹이 성장하면서, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다.
몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 '농장'을 이용한다.
각 로봇엔 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 진다.


### 9.1.6 별칭과 로봇 순환
올바른 자료 구조를 갖추었더라고 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주는 게 쉽지 않을 때도 있다.
한 URL이 또 다른 URL에 대한 별칭이라면 그 둘이 서도 달라 보이더라도 사실은 같은 리소스를 가리키고 있다.

### 9.1.7 URL 정규화하기
대부분의 웹 로봇은 URL들을 표준 형식으로 정규화 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다.
로봇은 아래와 같은 방법으로 모든 URL을 정규화된 형식으로 변환할 수 있다.

1. 포트 번호가 명시되지 않았다면 호스트 명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
3. # 태그들을 제거한다.

URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만 로봇들은 URL을 표준 형식으로 변환하는 것 만드로는 제거할 수 없는 다른 별칭을 만나게 될 것이다.

### 9.1.8 파일 시스템 랭크 순환
파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에 매우 교묘한 종류의 순환을 유발할 수 있다.


### 9.1.9 동적 가상 웹 공간
악의적인 웹 마스터들이 로봇들을 함정으로 빠뜨리기 위해 의도적으로 복잡한 크롤러 루프를 만드는 것은 있을 수 있는 일이다.

### 9.1.10 루프와 중복 피하기
모든 순환을 피하는 완벽한 방법은 없다.
잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
웹은 로봇이 문제를 일으킬 가능성으로 가득 차 있다.
이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.

***URL 정규화***
URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피한다.

***너비 우선 크롤링***
크롤러들은 언제든지 크롤링을 할 수 있는 URL들의 큰 집합을 갖고 있다.
방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면,
순환의 영향을 최소화 할 수 있다.

***스로틀링***
로봇이 웹 사이트에서 일정시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.

***URL 크기 제한***
로봇은 일정 길이를 넘는 URL의 크롤링은 거부할 수 있다.
만약 순환으로 인해 URL이 계속해서 길어진다면, 결국에는 길이 제한으로 인해 순환이 중단될 것이다.
주의해야 할 점은, 이 기법을 사용하면 가져오지 못하는 콘텐츠들도 틀림없이 있을 것이라는 점이다.

***URL/사이트 블랙리스트***
로봇 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL의 목록을 만들어 관리하고 그들을 전염병 피하듯 피한다.
문제를 일으키는 사이트나 URL이 발견될 때마다 이 블랙리스트에 추가한다.
***패턴 발견***
파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다.
***콘텐츠 지문***
지문은 더욱 복잡한 웹 크롤러들 몇몇에 의해 사용되는 중복을 감지하는 보다 직접적인 방법이다.
콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다. 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페이지의 링크는 크롤링 하지 않는다.
***사람의 모니터링***
로봇은 결국 어떤 기법으로도 해결할 수 없는 문제에 봉착할 것이다.
이러한 문제는 사람이 직접 제어해 주어야 한다.

## 9.2 로봇의 HTTP
로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않다.
그들 또한 HTTP 명세의 규칙을 지켜야 한다.
많은 로봇이 그들이 찾는 콘텐츠를 요청하기 위해 필요한 HTTP를 최소한으로만 구현하려 하기 때문에 많은 로봇이 HTTP/1.0 요청을 보낸다.

### 9.2 1 요청 헤더 식별하기
로봇들이 HTTP를 최소한도로만 지원하려고 함에도 불구하고 그들 대부분은 약간의 신원 식별 헤더를 구현하고 전송한다.
로봇 구현자들은 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇가지 헤더를 사이트에게 보내주는 것이 좋다.
이는 잘못된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려 할 때 모두 유용한 정보이다.
기본적인 식별 헤더들에는 다음과 같은 것이 있다.
- User-Agent
- From
- Accept
- Referer

### 9.2.2 가상 호스팅
로봇 구현자들은 Host 헤더를 지원할 필요가 있다.
가상 호스팅이 널리 퍼져있는 현실에서
요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.

대부분의 서버들은 기본적으로 특정 사이트 하나를 운영하도록 설정되어 있다.
따라서 Host 헤더를 포함하지 않은 크롤러는 두 개의 사이트를 운영하는 서버에 요청을 보내면
각각 다른 콘텐츠가 아닌 동일한 콘텐츠를 얻을 것이다.

### 9.2.3 조건부 요청
때때로 로봇들이 많은 양의 요청을 시도한다는 것을 고려할 때, 로봇이 검색하는 콘텐츠의 양을 최소화 하는 것은 상당히 의미 있는 일이다.
수십억 개의 웹페이지를 다운 받게 될 수도 있는 인터넷 검색엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것은 의미가 있다.

### 9.2.4 응답 다루기
대다수 로봇들은 주 관심사가 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에
응답 다루기라고 부를만한 일은 거의 하지 않지만
특정 몇몇 기능을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알 필요가 있다.

***상태 코드***
일반적으로 로봇들은 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.

또한 모든 서버가 언제나 항상 적절한 에러 코드를 반환하지 않는다는 걸 알아 두어야 한다.
***엔티티***
HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔티티 자체에 정보를 찾을 수 있다.
메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보이다.
이 태그는 수신자가 문서를 마치 그 문서의 HTTP 응답 값이 요청 헤더인
 Refresh 헤더를 포함하고 있는 것처럼 다루게 한다.


### 9.2.5 User-Agent 타겟팅
웹 관리자들은 많은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고,
그 로봇들로부터의 요청을 예상해야 한다.
많은 웹 사이트들은 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화한다.
이렇게 함으로써 사이트는 로봇에게는 콘텐츠 대신 에러 페이지를 제공한다.

사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다.
사이트 관리자들은 최소한 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록 대비해야 한다.

## 9.3 부적절하게 동작하는 로봇들
제멋대로 동작해 문제를 일으키는 로봇이 있다
몇가지 예를 알아보자

***폭주하는 로봇***
로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있다.
만약 로봇이 논리적 에러를 가지고 있거나 순환에 빠졋다면 웹 서버에 극심한 부하를 안겨 줄 것이며 이것이 서버에 과부하를 유발할 수 있다.
모든 로봇 저자들은 폭주 방지를 위한 보호 장치를 신경써서 설계해야 한다

***오래된 URL***
몇몇 로봇들은 URL의 목록을 방문한다.
해당 목록은 오래되었을 수 있고 로봇들은 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.
이는 에러 페이지를 제공하는 부하로 인해 서버의 요청에 대한 수용 능력이 감소할 수 있다.

***길고 잘못된 URL***
순환이나 프로그래밍 상의 오류로 인해 로봇은 웹 사이트에게 크고 의미없는 URL을 요청할 수 있다.
이는 웹 서버의 접근 로그를 어지럽게 채우고 허술한 웹 서버라면 고장을 일으킬 수 있다.

***호기심이 지나친 로봇***
어떤 로봇들은 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수도 있다.
웹에서 많은 양의 데이터를 검색하는 로봇의 구현자들은 사이트 구현자들이 인터넷을 통해 접근 가능하리라고 결코 의도하지 않았을 몇몇 특정 지점의 민감한 데이터를 그들의 로봇이 검색할 수 있다는 것에 주의해야 한다.

***동적 게이트웨이 접근***
로봇들이 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아니다.
만약 콘텐츠에 대한 URL로 요청한다면 처리 비용이 많이 들 것이다.

## 9.4 로봇 차단하기
로봇 커뮤니티는 로봇에 의한 웹 사이트 접근이 유발할 수 있는 문제를 알고 있었다.
표준명"Robots Exclusion Standard"라는 이름의 로봇의 동작을 제어하는 매커니즘 기법이 개발되었으며 이름을 따서 종종 robots.txt라고 불린다.

robos.txt의 아이디어는 단순하다. 어떤 웹 서버는 문서 루트에 robots.txt라고 이름 붙은 선택적인 파일을 제공할 수 있고 이 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다.
만약 어떤 로봇이 이 자발적인 표준에 따른다면 그것은 웹 사이트의 어떤 다른 리소스에 접근하기 전에 우선 그사이트의 robots.txt를 요청할 것이다.

### 9.4.1 로봇 차단 표준
로봇 차단 표준은 임시방편으로 마련된 표준이다. 이 표준이 작성되고 있을 때 이 표준을 소유하고 있는 주체가 없었고 업체들은 이 표준의 부분집합을 제각각 구현하고 있었다.
완벽하진 않지만 없는 것보다는 낫고 대부분의 주류 업체들과 검색엔진 크롤러들은 이 차단 표준을 지원한다.

| 버전 | 이름과 설명 | 날짜 |
|---|:---:|:---:|
| `0.0` | 로봇 배제 표준-Disallow 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 메커니즘 | 1994년 6월
| `1.0` | 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 코스터의 IETF 초안 | 1996년 11월
| `2.0` | 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너의 확장. 널리 지원되지 않음 | 1996년 11월

오늘날 대부분의 로봇들은 v0.0이나 v1.0 표준을 채택한다.

### 9.4.2 웹 사이트와 robots.txt 파일들
웹 사이트의 어떤 URL을 방문하기 전에 그 웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다.
호스트 명과 포트번호에 의해 정의되는 어떤 웹 사이트가 있을 때 그 사이트에 대한 robots.txt 파일은 단 하나만이 존재한다.

웹 마스터는 웹 사이트의 모든 콘텐츠에 대한 차단 규칙을 종합적으로 기술한 robots.txt 파일을 생성할 책임이 있다.

***robots.txt 가져오기***
로봇은 웹 서버의 여느 파일들과 마찬가지로 HTTP GET 메서드를 이용해 robots.txt 리소스를 가져온다.
파일이 존재한다면 text/plain 본문으로 반환한다.
만약 서버가 404 Not Found HTTP 상태 코드로 응답한다면 로봇은 그 서버는 로봇의 접근을 제한하지 않은 것으로 간주하고 어떤 파일이든 요청하게 될 것이다.

로봇은 사이트 관리자가 로봇의 접근을 추적할 수 있도록 From이나 User-Agent 헤더를 통해 신원 정보를 넘기고 사이트 관리자가 로봇에 대해 문의나 불만사항이 있을 경우를 위해 연락처를 제공해야 한다.

***응답 코드***
많은 웹 사이트가 robots.txt를 갖고 있지 않지만, 로봇은 그 사실을 모른다.
로봇은 어떤 웹 사이트든 반드시 robots.txt를 찾아보고 검색 결과에 따라 다르게 동작한다.
- 서버가 성공으로 응답하면 로봇은 반드시 그 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고 그 사이트에서 무언가를 가져오려 할 때 그 규칙에 따라야 한다.
- 만약 리소스가 존재하지 않는다고 서버가 응답하면(404) 로봇은 활성화된 차단 규칙이 존재하지 않는다 가정하고 robots.txt의 제약 없이 그 사이트에 접근할 수 있다.
- 만약 서버가 접근 제한으로 응답한다면(401 403) 로봇은 그 사이트로의 접근은 완전히 제한되어 있다 가정해야 한다
- 만약 요청 시도가 일시적으로 실패했다면(503) 로봇은 그 사이트의 리소스를 검색하는 것은 뒤로 미워야 한다.
- 만약 서버 응답이 리다이렉션을 의미한다면(3xx) 로봇은 리소스가 발견될 때까지 리다이렉트를 따라가야 한다.

### 9.4.3 robots.txt 파일 포맷
robots.txt 파일은 매우 단순한 줄 기반 문법을 갖는다.
robots.txt 파일의 각 줄은 빈 줄, 주석 줄, 규칙 줄의 세 가지 종류가 있다.
규칙 줄은 HTTP 헤더처럼 생겼고(<필드>:<값>) 패턴 매칭을 위해 사용된다.

각 레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 파일 끝 문자로 끝나며
특정 로봇들의 집합에 대한 차단 규칙의 집합을 기술한다.

레코드는 어떤 로봇이 이 레코드에 영향을 받는지 지정하는 하나 이상의 User-Agent 줄로 뒤이어 이 로봇들의 접근할 수 있는 URL들을 말해주는 Allow 줄과 Disallow 줄이 온다.

***User-Agent 줄***
각 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작하며 형식은 다음과 같다.
User-Agent: <root-name> or User-Agent: *
로봇의 이름은 로봇의 HTTP GET 요청 안의 User-Agent 헤더를 통해 보내진다.

robots.txt 파일을 처리한 로봇은 다음의 레코드에 반드시 복종해야 한다.
- 로봇 이름이 자신 이름의 부분 문자열이 될 수 있는 레코드들 중 첫 번째 것
- 로봇 이름이 '*'인 레코드들 중 첫 번째 것

만약 로봇이 자신의 이름에 대응하는 User-Agent 줄을 찾지 못하였고 와일드카드를 사용한 'Uset-Agent: *' 줄도 찾지 못했다면 대응하는 레코드가 없는 것이므로 접근에는 어떤 제한도 없다.
